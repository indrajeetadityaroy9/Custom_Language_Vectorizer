{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc3f1aa-6b6e-4953-a30d-a79ab0794170",
   "metadata": {
    "id": "cfc3f1aa-6b6e-4953-a30d-a79ab0794170"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c281567d-b99b-45df-8b1b-2b0e35cb9196",
   "metadata": {
    "id": "c281567d-b99b-45df-8b1b-2b0e35cb9196"
   },
   "source": [
    "The provided datasets were sub-sampled from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "The data was collected form IMDb movie reviews by webscraping text data from the website. \n",
    "(200 reviews, 54599 tokens in dev set, 1600 reviews, 425345 tokens in train set)\n",
    "(i.e. newswire, tweets, books, blogs, etc) Movie reviews\n",
    "Distribution of labels in the data (1: 105, 0: 95 in dev set, 1: 804, 0: 796 in train set)\n",
    "(8953 words in dev set, 30705 words in train set)\n",
    "6574 words overlap between train and dev set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64fd8f-f130-4f1b-8624-c151c502344b",
   "metadata": {
    "id": "2d64fd8f-f130-4f1b-8624-c151c502344b"
   },
   "source": [
    "Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e9bbb93-9a5d-4326-8dd0-d2ffcf242123",
   "metadata": {
    "id": "0e9bbb93-9a5d-4326-8dd0-d2ffcf242123"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.util import ngrams\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62c515ae-3637-42b6-9412-e0710e7821bb",
   "metadata": {
    "id": "62c515ae-3637-42b6-9412-e0710e7821bb"
   },
   "outputs": [],
   "source": [
    "def get_lists(input_file):\n",
    "    f=open(input_file, 'r')\n",
    "    lines = [line.split('\\t')[1:] for line in f.readlines()]\n",
    "    X = [row[0] for row in lines]\n",
    "    y=np.array([int(row[1]) for row in lines])\n",
    "    return X, y\n",
    "\n",
    "def filter_tokens(list, stopwords):\n",
    "    return [token for token in list.lower().split() if token not in stopwords and len(token) > 1]\n",
    "\n",
    "def generate_ngrams(tokens):\n",
    "    # Generate unigrams from tokens\n",
    "    return (' '.join(ngram) for n in range(1, 2) for ngram in ngrams(tokens, n))\n",
    "\n",
    "def apply_tfidf(tf_matrix):\n",
    "    # Calculate IDF and apply to TF matrix\n",
    "    num_docs, vocab_size = tf_matrix.shape\n",
    "    # Document frequency\n",
    "    df = np.bincount(tf_matrix.nonzero()[1], minlength=vocab_size)\n",
    "    # Inverse document frequency\n",
    "    idf = np.log((1 + num_docs) / (1 + df)) + 1\n",
    "    # Element-wise multiplication of TF matrix with IDF\n",
    "    tfidf_matrix = tf_matrix.multiply(idf).tocsr()\n",
    "    # L2 normalization\n",
    "    return normalize(tfidf_matrix, norm='l2', axis=1)\n",
    "\n",
    "def get_tfidf_vectors(token_lists, max_features=None, vocabulary=None):\n",
    "    stopwords_set = set(stopwords)\n",
    "    filtered_token_lists = [filter_tokens(token_list, stopwords_set) for token_list in token_lists]\n",
    "\n",
    "    if vocabulary is None:\n",
    "        # No pre-existing vocabulary supplied: Generate a new vocabulary from the corpus\n",
    "        # List of n-grams produced from the token lists\n",
    "        ngrams_list = (ngram for tokens in filtered_token_lists for ngram in generate_ngrams(tokens))\n",
    "        # Counter to count occurrences of each n-gram\n",
    "        vocabulary_counter = Counter(ngrams_list)\n",
    "        \n",
    "        if max_features is not None:\n",
    "            # Limit on the number of features is specified: Select only the top 'max_features' most common n-grams\n",
    "            high_freq_ngrams = vocabulary_counter.most_common(max_features)\n",
    "        else:\n",
    "            # No limit on the number of features: Utilizes all unique n-grams\n",
    "            high_freq_ngrams = vocabulary_counter.items()\n",
    "        # Sort the chosen n-grams alphabetically\n",
    "        sorted_vocab = sorted(ngram for ngram, _ in high_freq_ngrams)\n",
    "        # Create dictionary mapping each n-gram to a unique index\n",
    "        vocabulary = {ngram: i for i, ngram in enumerate(sorted_vocab)}\n",
    "    \n",
    "    # Initialize matrix (number of documents x vocabulary size)\n",
    "    matrix = lil_matrix((len(token_lists), len(vocabulary)), dtype=np.float64)\n",
    "    vocab_indices = {word: vocabulary[word] for word in vocabulary}\n",
    "    \n",
    "    for i, tokens in enumerate(token_lists):\n",
    "        # Generate n-grams from the tokens, filter them and count the frequency of each n-gram\n",
    "        token_counts = Counter(generate_ngrams(filter_tokens(tokens, stopwords_set)))\n",
    "        for token, count in token_counts.items():\n",
    "            # Retrieve the column index for the current token\n",
    "            j = vocab_indices.get(token)\n",
    "            if j is not None:\n",
    "                # If token is in the vocabulary update the matrix\n",
    "                matrix[i, j] = count\n",
    "    \n",
    "    # Convert TF matrix to TFIDF matrix\n",
    "    tfidf_matrix = apply_tfidf(matrix.tocsr())\n",
    "    return tfidf_matrix.toarray(), vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2e6c726-0e12-4e1f-8c8b-146a9a506a3f",
   "metadata": {
    "id": "e2e6c726-0e12-4e1f-8c8b-146a9a506a3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom TF-IDF Vectorizer\n",
      "Time taken:  0.3628091812133789  seconds\n",
      "\n",
      "sklearn's TF-IDF Vectorizer\n",
      "Time taken:  0.12508106231689453  seconds\n"
     ]
    }
   ],
   "source": [
    "# define constants for the files we are using\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "TEST_FILE = \"movie_reviews_test.txt\"\n",
    "\n",
    "train_corpus, y_train = get_lists(TRAIN_FILE)\n",
    "\n",
    "# First we will use our custom vectorizer to convert words to features, and time it.\n",
    "\n",
    "print(\"Custom TF-IDF Vectorizer\")\n",
    "start = time.time()\n",
    "custom_features, custom_vocabulary = get_tfidf_vectors(train_corpus)\n",
    "end = time.time()\n",
    "print(\"Time taken: \", end-start, \" seconds\")\n",
    "\n",
    "# Next we will use sklearn's TfidfVectorizer to load in the data, and time it.\n",
    "\n",
    "print(\"\\nsklearn's TF-IDF Vectorizer\")\n",
    "start = time.time()\n",
    "sklearn_vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "sklearn_features = sklearn_vectorizer.fit_transform(train_corpus)\n",
    "sklearn_vocabulary = sklearn_vectorizer.vocabulary_\n",
    "end = time.time()\n",
    "print(\"Time taken: \", end-start, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bdbf31c-bf6b-4ab4-b87b-47dd7c5468a8",
   "metadata": {
    "id": "8bdbf31c-bf6b-4ab4-b87b-47dd7c5468a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom TF-IDF Vectorizer vocabulary size: 43822\n",
      "sklearn TF-IDF Vectorizer vocabulary size: 22460\n",
      "Custom TF-IDF Features Sparsity (% of zeros): 99.75368821596459\n",
      "Sklearn TF-IDF Features Sparsity (% of zeros): 99.562071460374\n"
     ]
    }
   ],
   "source": [
    "#1.\n",
    "print(f\"Custom TF-IDF Vectorizer vocabulary size: {len(custom_vocabulary)}\")\n",
    "\n",
    "#2.\n",
    "print(f\"sklearn TF-IDF Vectorizer vocabulary size: {len(sklearn_vocabulary)}\")\n",
    "\n",
    "#5.\n",
    "custom_nz = np.count_nonzero(custom_features)\n",
    "custom_total = custom_features.shape[0] * custom_features.shape[1]\n",
    "custom_sparsity = (1 - custom_nz / custom_total) * 100\n",
    "print(\"Custom TF-IDF Features Sparsity (% of zeros):\", custom_sparsity)\n",
    "\n",
    "# 6.\n",
    "sklearn_nz = sklearn_features.nnz\n",
    "sklearn_total = sklearn_features.shape[0] * sklearn_features.shape[1]\n",
    "sklearn_sparsity = (1 - sklearn_nz / sklearn_total) * 100\n",
    "print(\"Sklearn TF-IDF Features Sparsity (% of zeros):\", sklearn_sparsity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463f022-7433-4397-88ed-eed8b01b1a45",
   "metadata": {
    "id": "6463f022-7433-4397-88ed-eed8b01b1a45"
   },
   "source": [
    "The vocabulary generated by the custom vectorizer is larger compared to the vocabulary generated by the sklearn TfidfVectorizer. The custom vectorizer implementation includes generating n-grams up to 2, encompassing both unigrams and bigrams, which utilizes individual words as well as combinations of words. In comparison, the default setting of the sklearn TfidfVectorizer only utilizes unigrams.The TFIDF implementation may also lead to a larger vocabulary produced by the custom vectorizer because its IDF computation adds smoothing terms to the log and an additional +1 to handle division by zero scenarios (np.log((1 + num_docs) / (1 + df)) + 1). This adjustment may affect the scaling of IDF values, increasing them and consequently enhancing the importance of all terms in general.In comparison, the sklearn TfidfVectorizer's IDF computation, only adds a smoothing term inside the logarithm effectively handling the importance of terms.\n",
    "\n",
    "N-grams were utilized because they capture more contextual information compared to individual tokens or unigrams, which assists in context-specific information tasks such as sentiment analysis using a prebuilt NLTK ngram generator to reduce memory consumption and increase processing efficiency.SciPy lil_matrix is utilized for constructing the TF matrix as it allows for more efficient additions of terms where the matrix dimensions are not predefined and for non-sequential updates. For the TFIDF calculation, SciPy sparse is utilized for directly multiplying the IDF by the TF, without converting the sparse matrix into a dense format. L2 normalization is utilized for handling variable-lengths in similarity computations as it scales each term by the overall length of the token list, enabling a more accurate comparison of variable length token lists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c76612-4dd4-489c-90e4-0c38029c32d1",
   "metadata": {
    "id": "09c76612-4dd4-489c-90e4-0c38029c32d1"
   },
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68eb6d36-1e71-4665-aab4-1217a42e6dcc",
   "metadata": {
    "id": "68eb6d36-1e71-4665-aab4-1217a42e6dcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom TFIDF Vectorizer - Accuracy: 0.8200, F1 Score: 0.8302\n",
      "Sklean TFIDF Vectorizer - Accuracy: 0.8150, F1 Score: 0.8295\n"
     ]
    }
   ],
   "source": [
    "# First use sklearn's LogisticRegression classifier to do sentiment analysis using your custom feature vectors:\n",
    "custom_classifier = LogisticRegression()\n",
    "\n",
    "# Load the test data, extract features using your custom vectorizer, and test the performance of the LR classifier\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "TEST_FILE = \"movie_reviews_test.txt\"\n",
    "train_corpus, y_train = get_lists(TRAIN_FILE)\n",
    "test_corpus, y_test = get_lists(TEST_FILE)\n",
    "\n",
    "custom_features, custom_vocabulary = get_tfidf_vectors(train_corpus)\n",
    "custom_test_features, _ = get_tfidf_vectors(test_corpus, vocabulary=custom_vocabulary)\n",
    "\n",
    "custom_classifier.fit(custom_features, y_train)\n",
    "custom_predictions = custom_classifier.predict(custom_test_features)\n",
    "\n",
    "\n",
    "# Print the accuracy of your model on the test data\n",
    "custom_accuracy = accuracy_score(y_test, custom_predictions)\n",
    "f1_custom = f1_score(y_test, custom_predictions)\n",
    "\n",
    "print(f\"Custom TFIDF Vectorizer - Accuracy: {custom_accuracy:.4f}, F1 Score: {f1_custom:.4f}\")\n",
    "\n",
    "# Now repeat the above steps, but this time using features extracted by sklearn's Tfidfvectorizer\n",
    "sklearn_classifier = LogisticRegression()\n",
    "\n",
    "sklearn_vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "sklearn_features = sklearn_vectorizer.fit_transform(train_corpus)\n",
    "sklearn_test_features = sklearn_vectorizer.transform(test_corpus)\n",
    "\n",
    "sklearn_classifier.fit(sklearn_features, y_train)\n",
    "sklearn_predictions = sklearn_classifier.predict(sklearn_test_features)\n",
    "\n",
    "sklearn_accuracy = accuracy_score(y_test, sklearn_predictions)\n",
    "sklearn_f1 = f1_score(y_test, sklearn_predictions)\n",
    "\n",
    "print(f\"Sklean TFIDF Vectorizer - Accuracy: {sklearn_accuracy:.4f}, F1 Score: {sklearn_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f995a09c-0447-422d-8b37-e9120cfadfab",
   "metadata": {
    "id": "f995a09c-0447-422d-8b37-e9120cfadfab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom TFIDF Vectorizer - Accuracy: 0.7850, F1 Score: 0.8037\n",
      "Sklean TFIDF Vectorizer - Accuracy: 0.8050, F1 Score: 0.8219\n"
     ]
    }
   ],
   "source": [
    "# First use sklearn's LogisticRegression classifier to do sentiment analysis using your custom feature vectors:\n",
    "custom_classifier = LogisticRegression()\n",
    "\n",
    "# Load the test data, extract features using your custom vectorizer, and test the performance of the LR classifier\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "TEST_FILE = \"movie_reviews_test.txt\"\n",
    "train_corpus, y_train = get_lists(TRAIN_FILE)\n",
    "test_corpus, y_test = get_lists(TEST_FILE)\n",
    "\n",
    "custom_features, custom_vocabulary = get_tfidf_vectors(train_corpus, max_features=1000)\n",
    "custom_test_features, _ = get_tfidf_vectors(test_corpus, vocabulary=custom_vocabulary)\n",
    "\n",
    "custom_classifier.fit(custom_features, y_train)\n",
    "custom_predictions = custom_classifier.predict(custom_test_features)\n",
    "\n",
    "\n",
    "# Print the accuracy of your model on the test data\n",
    "custom_accuracy = accuracy_score(y_test, custom_predictions)\n",
    "f1_custom = f1_score(y_test, custom_predictions)\n",
    "\n",
    "print(f\"Custom TFIDF Vectorizer - Accuracy: {custom_accuracy:.4f}, F1 Score: {f1_custom:.4f}\")\n",
    "\n",
    "# Now repeat the above steps, but this time using features extracted by sklearn's Tfidfvectorizer\n",
    "sklearn_classifier = LogisticRegression()\n",
    "\n",
    "sklearn_vectorizer = TfidfVectorizer(stop_words=stopwords, max_features=1000)\n",
    "sklearn_features = sklearn_vectorizer.fit_transform(train_corpus)\n",
    "sklearn_test_features = sklearn_vectorizer.transform(test_corpus)\n",
    "\n",
    "sklearn_classifier.fit(sklearn_features, y_train)\n",
    "sklearn_predictions = sklearn_classifier.predict(sklearn_test_features)\n",
    "\n",
    "sklearn_accuracy = accuracy_score(y_test, sklearn_predictions)\n",
    "sklearn_f1 = f1_score(y_test, sklearn_predictions)\n",
    "\n",
    "print(f\"Sklean TFIDF Vectorizer - Accuracy: {sklearn_accuracy:.4f}, F1 Score: {sklearn_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2858e409-8a87-43aa-8646-1e5419cce6db",
   "metadata": {
    "id": "2858e409-8a87-43aa-8646-1e5419cce6db"
   },
   "source": [
    "The difference between the custom TFIDF vectorizer and the sklearn TFIDF vectorizer results is minimal indicating that both vectorizers are similarly effective when selecting the 1000 most relevant terms from the data.\n",
    "\n",
    "According to sklearn's documentation for the Tfidfvectorizer, the smoothing parameter smooth_idf (default behavior True) essentially increases the document frequency by 1 preventing division by zero and reducing variance in IDF values, consequently allowing the vectorizer to not be biased towards high frequency terms and able to pick up outlier or uncommon terms, making the vectorization process more robust.As high frequency terms do not bias or skew the feature weights, classification performance is more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c652f8ab-3893-4021-b33f-c466b2e1d98a",
   "metadata": {
    "id": "c652f8ab-3893-4021-b33f-c466b2e1d98a"
   },
   "source": [
    "Feedforward Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f563ec2-d02a-4176-908c-011acd8851de",
   "metadata": {
    "id": "3f563ec2-d02a-4176-908c-011acd8851de"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.sparse import issparse\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# if torch.backends.mps.is_available():\n",
    "# \tdevice = torch.device(\"mps\")\n",
    "if torch.cuda.is_available():\n",
    "\tdevice = torch.device(\"cuda\")\n",
    "else:\n",
    "\tdevice = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcb6aa45-e33a-4d08-bff8-996e1f80dad2",
   "metadata": {
    "id": "fcb6aa45-e33a-4d08-bff8-996e1f80dad2"
   },
   "outputs": [],
   "source": [
    "class feedforward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # First fully connected layer with input size 10000 and output size 512\n",
    "        self.fc1 = nn.Linear(10000, 512)\n",
    "        # Activation function preventing dead neurons\n",
    "        self.act1 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        # Batch normalization for first layer to stabilize learning by normalizing the layer input\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        # Dropout layer to reduce overfitting by randomly setting some activations to zero\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        # Second fully connected layer reducing dimension from 512 to 256\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        # Batch normalization for second layer\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        # Dropout layer to further reduce overfitting\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        # Final fully connected layer that outputs to a single unit\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Processing input through the first layer\n",
    "        X = self.fc1(X)\n",
    "        # Applying batch normalization\n",
    "        X = self.bn1(X)\n",
    "        # Activation function\n",
    "        X = self.act1(X)\n",
    "        # Applying dropout\n",
    "        X = self.dropout1(X)\n",
    "        # Second layer processing\n",
    "        X = self.fc2(X)\n",
    "        # Second layer batch normalization\n",
    "        X = self.bn2(X)\n",
    "        # Activation function for the second layer\n",
    "        X = relu(X)\n",
    "        # Second dropout\n",
    "        X = self.dropout2(X)\n",
    "        # Final layer processing to produce output\n",
    "        X = self.fc3(X)\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward pass through the network\n",
    "        X = self.forward(X)\n",
    "        # Applying sigmoid activation to output probabilities\n",
    "        return torch.sigmoid(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba08e134-2a81-48b9-89c3-43046be4cc1b",
   "metadata": {
    "id": "ba08e134-2a81-48b9-89c3-43046be4cc1b"
   },
   "outputs": [],
   "source": [
    "# Load the data using custom and sklearn vectors\n",
    "\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "TEST_FILE = \"movie_reviews_test.txt\"\n",
    "VALIDATION_FILE = \"movie_reviews_dev.txt\"\n",
    "\n",
    "train_corpus, y_train =  get_lists(TRAIN_FILE)\n",
    "test_corpus, y_test = get_lists(TEST_FILE)\n",
    "validation_corpus, y_validation = get_lists(VALIDATION_FILE)\n",
    "\n",
    "custom_features, custom_vocabulary = get_tfidf_vectors(train_corpus, max_features=10000)\n",
    "custom_features_validation, _ = get_tfidf_vectors(validation_corpus, vocabulary=custom_vocabulary)\n",
    "custom_features_test, _ = get_tfidf_vectors(test_corpus, vocabulary=custom_vocabulary)\n",
    "\n",
    "sklearn_vectorizer = TfidfVectorizer(stop_words=stopwords, max_features=10000)\n",
    "sklearn_features = sklearn_vectorizer.fit_transform(train_corpus)\n",
    "sklearn_features_validation = sklearn_vectorizer.transform(validation_corpus)\n",
    "sklearn_features_test = sklearn_vectorizer.transform(test_corpus)\n",
    "sklearn_vocabulary = sklearn_vectorizer.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "055e1e59-4d96-4a6b-8e16-cd14fcc4b760",
   "metadata": {
    "id": "055e1e59-4d96-4a6b-8e16-cd14fcc4b760"
   },
   "outputs": [],
   "source": [
    "custom_model = feedforward()\n",
    "custom_model_loss_fn = nn.BCEWithLogitsLoss()\n",
    "custom_model_optimizer = optim.Adam(custom_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "custom_model_scheduler = ReduceLROnPlateau(custom_model_optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "sklearn_model = feedforward()\n",
    "sklearn_model_loss_fn = nn.BCEWithLogitsLoss()\n",
    "sklearn_model_optimizer = optim.Adam(sklearn_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "sklearn_model_scheduler = ReduceLROnPlateau(sklearn_model_optimizer, 'min', patience=5, factor=0.5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "237cbc15-3473-427d-8d3f-af1059f736b4",
   "metadata": {
    "id": "237cbc15-3473-427d-8d3f-af1059f736b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 0.0009568023291649297\n",
      "Epoch 1: Validation Loss = 0.608829357794353\n",
      "Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 2: Training Loss = 0.001053819251828827\n",
      "Epoch 2: Validation Loss = 0.5844955359186444\n",
      "Epoch 3: Training Loss = 0.0016933093522675335\n",
      "Epoch 3: Validation Loss = 0.5978603192738124\n",
      "Epoch 4: Training Loss = 0.000992202702909708\n",
      "Epoch 4: Validation Loss = 0.6012100534779685\n",
      "Epoch 5: Training Loss = 0.0010565883934032171\n",
      "Epoch 5: Validation Loss = 0.596877498286111\n",
      "Epoch 6: Training Loss = 0.0008121662749908864\n",
      "Epoch 6: Validation Loss = 0.5838739063058581\n",
      "Epoch 7: Training Loss = 0.0009749176539480686\n",
      "Epoch 7: Validation Loss = 0.6092507881777627\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 8: Training Loss = 0.0010520043026190252\n",
      "Epoch 8: Validation Loss = 0.6309189328125545\n",
      "Epoch 9: Training Loss = 0.0007125991614884697\n",
      "Epoch 9: Validation Loss = 0.6017114009175982\n",
      "Epoch 10: Training Loss = 0.0007751183141954243\n",
      "Epoch 10: Validation Loss = 0.6062722504138947\n",
      "Epoch 11: Training Loss = 0.0008210917341057211\n",
      "Epoch 11: Validation Loss = 0.6007213720253536\n",
      "Early stopping triggered after 11 epochs.\n",
      "Epoch 1: Training Loss = 0.0012504681100836024\n",
      "Epoch 1: Validation Loss = 0.5683901757001877\n",
      "Epoch 2: Training Loss = 0.0009810105885844677\n",
      "Epoch 2: Validation Loss = 0.5625503957271576\n",
      "Epoch 3: Training Loss = 0.0014615723135648294\n",
      "Epoch 3: Validation Loss = 0.5758056959935597\n",
      "Epoch 4: Training Loss = 0.0010075282235629856\n",
      "Epoch 4: Validation Loss = 0.5756182159696307\n",
      "Epoch 5: Training Loss = 0.0009019186737714335\n",
      "Epoch 5: Validation Loss = 0.5681098699569702\n",
      "Epoch 6: Training Loss = 0.0009519791626371444\n",
      "Epoch 6: Validation Loss = 0.5700574389525822\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 7: Training Loss = 0.0007419518480310216\n",
      "Epoch 7: Validation Loss = 0.5711204367024558\n",
      "Early stopping triggered after 7 epochs.\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 50 epochs on both custom and sklearn vectors\n",
    "def create_data_loader(features, labels, batch_size, shuffle, num_workers=4):\n",
    "    # Create a dataset from tensors. Converts features and labels into tensors.\n",
    "    tensor_dataset = TensorDataset(torch.tensor(features, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32).unsqueeze(1))\n",
    "    # Return a DataLoader which is an iterable over the dataset.\n",
    "    return DataLoader(tensor_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "\n",
    "def sparse_matrix_to_tensor(matrix):\n",
    "    # Check if the input matrix is a sparse matrix.\n",
    "    if issparse(matrix):\n",
    "        # Convert the sparse matrix to a dense array\n",
    "        dense_array = matrix.toarray()\n",
    "        # Convert the dense array into a torch tensor\n",
    "        return torch.tensor(dense_array, dtype=torch.float32)\n",
    "    else:\n",
    "        # Convert the dense array into a torch tensor\n",
    "        return torch.tensor(matrix, dtype=torch.float32)\n",
    "\n",
    "def train_model(train_loader, validation_loader, model, loss_fn, optimizer, device, scheduler, epochs, patience):\n",
    "    model.to(device)\n",
    "\n",
    "    validation_loss_limit = float('inf')  # Initialize the best validation loss to infinity for comparison\n",
    "    early_stop_limit = 0  # Counter to track the number of epochs without improvement\n",
    "\n",
    "    for epoch in range(epochs):  # Loop over the dataset multiple times\n",
    "        model.train()\n",
    "        train_loss = 0  # Initialize loss for the epoch\n",
    "\n",
    "        for inputs, labels in train_loader:  # Iterate over batches of data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the same device as the model\n",
    "            optimizer.zero_grad()  # Clear gradients before each backward pass\n",
    "            outputs = model(inputs)  # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "            loss = loss_fn(outputs, labels)  # Calculate the loss based on model output and real labels\n",
    "            loss.backward()  # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            optimizer.step()  # Perform a single optimization step (parameter update)\n",
    "            train_loss += loss.item()  # Sum up losses for the epoch\n",
    "\n",
    "        train_loss /= len(train_loader)  # Calculate average training loss over the epoch\n",
    "        print(f'Epoch {epoch+1}: Training Loss = {train_loss}')\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        validation_loss = 0  # Initialize loss for validation\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in validation_loader:  # Iterate over validation data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)  # Forward pass\n",
    "                validation_loss += loss_fn(outputs, labels).item()  # Accumulate the validation loss\n",
    "\n",
    "        validation_loss /= len(validation_loader)  # Calculate average validation loss\n",
    "        print(f'Epoch {epoch+1}: Validation Loss = {validation_loss}')\n",
    "\n",
    "        # Adjust the learning rate based on the validation loss\n",
    "        scheduler.step(validation_loss)\n",
    "\n",
    "        # Check for improvement in validation loss to decide on early stopping\n",
    "        if validation_loss < validation_loss_limit:\n",
    "            validation_loss_limit = validation_loss  # Update the best found validation loss\n",
    "            early_stop_limit = 0  # Reset early stopping counter\n",
    "        else:\n",
    "            early_stop_limit += 1  # Increment the counter when no improvement\n",
    "\n",
    "        if early_stop_limit >= patience:  # If no improvement for 'patience' consecutive epochs, stop training\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "    return model\n",
    "\n",
    "custom_features_tensor = sparse_matrix_to_tensor(custom_features)\n",
    "custom_features_validation_tensor = sparse_matrix_to_tensor(custom_features_validation)\n",
    "custom_features_test_tensor = sparse_matrix_to_tensor(custom_features_test)\n",
    "\n",
    "custom_train_dataset = TensorDataset(custom_features_tensor, torch.tensor(y_train, dtype=torch.float32).unsqueeze(1))\n",
    "custom_validation_dataset = TensorDataset(custom_features_validation_tensor, torch.tensor(y_validation, dtype=torch.float32).unsqueeze(1))\n",
    "custom_test_dataset = TensorDataset(custom_features_test_tensor, torch.tensor(y_test, dtype=torch.float32).unsqueeze(1))\n",
    "\n",
    "custom_train_loader = DataLoader(custom_train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "custom_validation_loader = DataLoader(custom_validation_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "custom_test_loader = DataLoader(custom_test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "sklearn_features_tensor = sparse_matrix_to_tensor(sklearn_features)\n",
    "sklearn_features_validation_tensor = sparse_matrix_to_tensor(sklearn_features_validation)\n",
    "sklearn_features_test_tensor = sparse_matrix_to_tensor(sklearn_features_test)\n",
    "\n",
    "sklearn_train_dataset = TensorDataset(sklearn_features_tensor, torch.tensor(y_train, dtype=torch.float32).unsqueeze(1))\n",
    "sklearn_validation_dataset = TensorDataset(sklearn_features_validation_tensor, torch.tensor(y_validation, dtype=torch.float32).unsqueeze(1))\n",
    "sklearn_test_dataset = TensorDataset(sklearn_features_test_tensor, torch.tensor(y_test, dtype=torch.float32).unsqueeze(1))\n",
    "\n",
    "sklearn_train_loader = DataLoader(sklearn_train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "sklearn_validation_loader = DataLoader(sklearn_validation_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "sklearn_test_loader = DataLoader(sklearn_test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "trained_custom_model = train_model(\n",
    "    train_loader=custom_train_loader,\n",
    "    validation_loader=custom_validation_loader,\n",
    "    model=custom_model,\n",
    "    loss_fn=custom_model_loss_fn,\n",
    "    optimizer=custom_model_optimizer,\n",
    "    device=device,\n",
    "    scheduler=custom_model_scheduler,\n",
    "    epochs=50,\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "trained_sklearn_model = train_model(\n",
    "    train_loader=sklearn_train_loader,\n",
    "    validation_loader=sklearn_validation_loader,\n",
    "    model=sklearn_model,\n",
    "    loss_fn=sklearn_model_loss_fn,\n",
    "    optimizer=sklearn_model_optimizer,\n",
    "    device=device,\n",
    "    scheduler=sklearn_model_scheduler,\n",
    "    epochs=50,\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdf1319f-41c0-4237-982f-b60d492fbbd7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdf1319f-41c0-4237-982f-b60d492fbbd7",
    "outputId": "187d0ec2-1f99-417e-b23d-de234adb2848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torcheval in /Users/indrajeetadityaroy/Library/Python/3.11/lib/python/site-packages (0.0.7)\n",
      "Requirement already satisfied: typing-extensions in /Users/indrajeetadityaroy/Library/Python/3.11/lib/python/site-packages (from torcheval) (4.11.0)\n",
      "Custom TFIDF Vectorizer - F1 Score: 0.8169, AUROC: 0.8911, Accuracy: 0.8050\n",
      "Sklean TFIDF Vectorizer - F1 Score: 0.8195, AUROC: 0.8903, Accuracy: 0.8150\n"
     ]
    }
   ],
   "source": [
    "!pip install torcheval\n",
    "\n",
    "from torcheval.metrics.functional import binary_f1_score\n",
    "from torcheval.metrics import BinaryAUROC, BinaryAccuracy\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    auroc_fn = BinaryAUROC()\n",
    "    accuracy_fn = BinaryAccuracy()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.squeeze(1)\n",
    "            outputs = model(inputs)\n",
    "            probabilities = torch.sigmoid(outputs).squeeze(1)\n",
    "            predicted_labels = torch.round(probabilities)\n",
    "\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            accuracy_fn.update(predicted_labels, labels)\n",
    "            auroc_fn.update(probabilities, labels)\n",
    "\n",
    "    f1_score_ = binary_f1_score(torch.tensor(predictions), torch.tensor(true_labels))\n",
    "\n",
    "    auroc = auroc_fn.compute()\n",
    "    accuracy = accuracy_fn.compute()\n",
    "\n",
    "    return f1_score_.item(), auroc.item(), accuracy.item()\n",
    "\n",
    "f1_custom, auroc_custom, accuracy_custom = evaluate_model(trained_custom_model, custom_test_loader, device)\n",
    "f1_sklearn, auroc_sklearn, accuracy_sklearn = evaluate_model(trained_sklearn_model, sklearn_test_loader, device)\n",
    "\n",
    "print(f\"Custom TFIDF Vectorizer - F1 Score: {f1_custom:.4f}, AUROC: {auroc_custom:.4f}, Accuracy: {accuracy_custom:.4f}\")\n",
    "print(f\"Sklean TFIDF Vectorizer - F1 Score: {f1_sklearn:.4f}, AUROC: {auroc_sklearn:.4f}, Accuracy: {accuracy_sklearn:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
